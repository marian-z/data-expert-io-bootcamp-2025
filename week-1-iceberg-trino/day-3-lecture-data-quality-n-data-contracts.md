# Apache Iceberg & Data Contracts

**DE.IO Description:**

In today’s lecture, Zach dives into the essential concept of data contracts, explaining how they build trust between data producers and consumers by establishing clear guidelines for data generation and usage. Understanding data contracts is key to making informed decisions and preventing data misuse. It is a must know for anyone in data engineering!

## Main Topics for the lecture

- What is a data contract?
- What are the different ways data contracts are implemented in big tech?
- Iceberg branching model
- Limits to Iceberg time-travel feature

## Data Contracts

- This term has a lot of hype, that has somewhat destroyed the concept in a way that multiple vendors offer some kind of data contract solution that often times does not get correctly implemented and thus destroys trust that is already week in the data world between producers and consumers
- Data contract is:
  - An agreement between producers and consumers
  - It should include every aspect of expectations
- Technicals:
  - How and what data is generated
- Business:
  - How and why data is being used
- When creating data contracts you should think of all the expectations that data consumer could have so as much can be included in the contract
- Data engineers are both producers and consumers
- **What is in the data contract:**
  - **Basics (almost everybody does this explicitly or implicitly):**
    - Data schema
    - Table names
      - Don’t underestimate this because the name of datasets has an impact on how they are discovered and used
    - Data stores
      - Where the data lives also dictates how the data is used
      - If you have data in Redis it is most likely going to be used in applications
      - The data store kind of tells you its use case
  - **Intermediate (still the bare minimum in some regards):**
    - How does data show up in production?
      - Is it an INSERT statement, is it real-time data, daily etc.
    - When should the data be expected in production?
      - Very important to have this set up in the contract so you don’t waste your time talking to Data Analysts asking you this question all over again
    - How long should we hold onto this data & when will we anonymize it?
      - Depends a lot on where you live - Europe has different data regulations than the US etc.
      - Data Analysts will almost always want you to hold onto all the data that you possibly can
      - _When asked to hold as much data as you can, aggregating the data might be one way to go, because this helps solve the issue with PII information in the data (personally identifiable information) + the aggregated data will not take up as much space. It solves PII issue by not having the data at the user_id level once aggregated for example_
      - _Another approach might be to anonymize the data so that way you do not have to worry about keeping the data at user_id level_
  - **Advanced (making stakeholders actually agree to things too):**
  - Basic and Intermediate part has been mostly very technical and directly related to the pipeline
  - It is important to keep the 3 things below in mind because after all the data contract is a collaborative effort between you and your stakeholders and producers
  - Some of these things below you should answer - i.e. between you and software engineer that is producing the data for you to use
  - _Always remember to ask these questions about why should you create a pipeline and how it brings value!_
    - What is the ROI being generated by this data?
    - What decisions are being made with this data?
    - What surfaces are exposing this data?
- Doing all of the upfront work mentioned in the data contract above (mainly the questions in the Advanced part) will save you so much suffering from creating low-value, hard-to-maintain data pipelines
- It forces you to:
  - think about the business impact
  - think long-term
  - to think about maintenance
  - to think about quality
  - to think about trust
- **It will help you get promoted so much faster when you learn to always keep these things in mind!**

##

### **How does data show up in production?**

#### **WAP Pattern**

- - 1\. Write to a table with identical schema as production
    - 2\. Audits on the table
    - 3\. Publish the data into production, if audits pass
- This pattern used to require creating a throw-away staging table that we no longer need. That is no longer required because of Iceberg!

![WAP_Pattern](https://github.com/marian-z/data-expert-io-bootcamp-2025/raw/main/week-1-iceberg-trino/images/lecture_3_wap_pattern.png)

- With Iceberg you can treat a table as a Git repository
  - ALTER TABLE example.table CREATE BRANCH merge_branch;
- If the audit passes then you simply promote the branch to MAIN and thats how the data gets to production
  - CALL syste.fast_forward(‘example.table’, ‘main’, ‘merge_branch’)
  - This used to be INSERT INTO statement when we did not have this option
- Remember to clean up the old branch
  - ALTER TABLE example.table DROP BRANCH merge_branch;
- Alerts about failed audit usually get passed as some sort of alert in email or such
- Examples of audit checks:
  - Blocking:
    - Duplicates (blocking because it will probably break something downstream)
  - Non-blocking:
    - Row-count (if we have 50% more data than it should be probably fine, however if its 200% more data that would probably be a blocking check that we would need to look into)
- The main reason why you follow this pattern at all is to prevent downstream pipelines from starting with bad data

#### **Signal Table Pattern**

- Used previously at Facebook (when Zach was there)
  - 1\. Write directly to production
  - 2\. Audit the table
  - 3\. Publish the signal
    - Downstream pipelines wait for that

![Signal_Table_Pattern](https://github.com/marian-z/data-expert-io-bootcamp-2025/raw/main/week-1-iceberg-trino/images/lecture_3_signal_table_pattern.png)

- It is simpler than WAP + very fast and efficient, but with increased risk because you publish straight to production
- **Iceberg has changed the game where WAP is now a supreme pattern because you don’t need the staging table anymore since you can publish the data straight into the production table but in the “staging” branch and do the audit checks there**

## Limits to Time Travel

- You shouldn’t hold onto every ICEBERG SNAPSHOT because:
  - Best case scenario:
    - You’re making Jeff Bezos richer than he needs to be
  - Worst case scenario:
    - The EU GDPR guys knock on your door and you owe them 5 billion dollars
- Example query to expire snapshot/s:
  - CALL system.expire_snapshots(table=> ‘bootcamp.nba_player_season’, older_than => TIMESTAMP ‘2025-01-01 00:00:00’, retain_last=>1)
- Time travel is both benefit and risk because on one hand you can redo some of your mistakes and have a better overview of the past data movements etc., but on the other hand you can run into compliance risks because unless you delete a certain snapshot the data (i.e. PII) is still technically there
- **Week 1 Lab 3 content:**
  - WAP Pattern in PySpark and Iceberg
  - How to query branched data and create useful audits with Trino

## Live Q&A

Q: Are blocking and non-blocking audits part of a data contract?

A: Yes.

Q: How is the signal for the Signal Table pattern generated?

A: Manually generated by having a separate signal_ table that publishes an empty partition which you can then catch with PartitionSensor in Airflow.

Q: Are you typically branching, etc. with manual queries, or in some more formal CI/CD process.

A: Definitely as a part of the CI/CD process.

Q: Any scenario where you would prefer Signal Table over WAP while using Iceberg?

A: Not anymore with the branching capabilities.

Q: What kind of sign-off do you need on data contracts? Does that include stakeholders?

A: Depends on the company but from Zach's experience it has been: Primary and Secondary Pipeline Owner, Metric (Analytics) Primary and Secondary Owner + One Data Architect (preferably not involved in the process so you have some kind of outside insight.

Q: Do Data Contacts evolve as business needs change?  
A: Definitely, they are living breathing documents that need updating.

Q: How is time-travel different from branching in Iceberg, is it the same thing?

A: They are 2 different things that can be used together.

Q: Do the Iceberg branches have snapshots too?  
A: Yes but it is not advised to use it extensively due to mentioned compliance issues.

## Lab Notes

- For daily batch loads you would create a branch each day and drop it after successful audit and merge into main branch of the table
- In order to do something like a git revert merge we could simply time travel to a snapshots that is the previous version of the table
- Table branches do not automatically expire, we have to do that manually or change maximum ref age value for the table
- Data engineering is becoming software engineering and it's going to become so important to be able to think of it that way (version control, best practices etc.)
